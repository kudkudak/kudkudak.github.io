---
layout: default
title: Home
permalink: index.html
---

<p>Hello! I am currently the Chief Technology Officer and Chief Scientist at <a href="http://molecule.one"> Molecule.one</a>, a biotech startup that combines in a closed loop high-throughput organic chemistry laboratory with machine learning models. I am passionate about improving fundamental aspects of deep learning and how it can be used to empower scientific discovery. I am also a Venture Advisor at <a href="https://expeditionsfund.com/">Expeditions Fund</a> where I help select AI startups for investment.</p>

<p> If you are looking for an advisor for your startup or PhD advisor, please feel free to reach out! </p>

<p> In 2024, I am teaching Automated Scientific Discovery using Deep Learning course at Jagiellonian University. </p>

<p> In my scientific work, I have largely focused on the role of optimization in the success of deep learning. My main contribution is largely resolving the mystery why learning rate, a fundamental hyperparameter, impacts generalization in deep learning. In the process we have discovered that all deep learning trainings go through chaotic phase in the beginning of training. This in particular has been part of <a href="https://mitpress.mit.edu/9780262048644/understanding-deep-learning">Understanding Deep Learning</a> book (Prince, MIT press). For more details see our ICLR 2020 <a href="https://arxiv.org/abs/2002.09572">paper</a> (spotlight). This is closely related to the <a hreh="https://openreview.net/pdf?id=jh-rTtvkGeM">Edge of Stability</a> phenomenon.</p> 
    
<p> I completed a post-doc with Kyunghyun Cho and Krzysztof Geras at New York University, and was also an an Assistant Professor at Jagiellonian University (member of <a href="http://gmum.net">GMUM.net</a>). I received my PhD from Jagiellonian University co-supervised by  <a href="http://gmum.net/index.php?m=team&user=tabor">Jacek Tabor</a> and <a href="http://homepages.inf.ed.ac.uk/amos/">Amos Storkey</a> (University of Edinburgh). During PhD, I closely collaborated as a visiting researcher with <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>, and with Google Research in Zurich.  </p> 

<p> I do my best to contribute to the broad machine learning community. Currently, I serve as an Action Editor for <a href="https://www.jmlr.org/tmlr/">TMLR</a> and an area chair for ICLR 2023 (before that NeurIPS 2020-23, ICML 2020-22, ICLR 2020-22).  </p>

<p> My email is staszek.jastrzebski (on gmail). </p> 


<p> News </p>
<ul>
    <li> (5.2022) <a href="https://arxiv.org/abs/2202.05306">"Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks</a> accepted to ICML 2022.
    <li> (4.2022) <a href="https://www.nature.com/articles/s41598-022-10526-z">"Differences between human and machine perception in medical diagnosis"</a> accepted to Nature Scientific Reports! We show early evidence that a radiologist-level deep nets could have learned novel biomarkers for breast cancer screening.
    <li> (6.2021) <a href="https://arxiv.org/abs/2012.14193">"Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization"</a> accepted to ICML 2021.
    <li> (6.2021) I was awarded the <a href="https://www.fnp.org.pl/konkurs-start-2021-rozstrzygniety/">START</a> stipend for our work on understanding optimization in deep learning. The stipend is given to the top 100 young scientists (selected from around 1000) in Poland. 
    <li> (6.2021) Molecule.one has closed 4.6$M seed round, read news on <a href="https://techcrunch.com/2021/06/01/molecule-one-grows-its-drug-synthesis-ai-platform-with-a-4-6m-seed-round/">TechCrunch</a>!
    <li> (5.2021) Our <a href="https://arxiv.org/abs/2012.14193">Catastrophic Fisher Explosion</a> has been accepted to ICML!
    <li> (5.2021) Huggingmolecules package including ours Molecule Attention Transformer is <a href="https://github.com/gmum/huggingmolecules">online</a>.
    <li> (4.2021) I have been inclued in the top ten nominees for <a href="https://weareplug.in/api/">Ambassadors of Polish Innovation Award</a>.
<!--     <li> (11.2020) Started as Chief Scientific Officer at Molecule.one and Assistant Professor at Jagiellonian University 
    <li> (3.2020) Our <a href="https://openreview.net/forum?id=aD86B9pZ6u">paper</a> on multimodal learning for breast cancer screening has been accepted to MIDL 2020 as a Spotlight! </li>
    <li> (12.2019) Our <a href="https://iclr.cc/virtual_2020/poster_r1g87C4KwB.html">paper</a> on the early phase of the optimization trajectory has been accepted to ICLR 2020 as a Spotlight! </li>
    <li> (09.2019) Large Scale Structure of Neural Networks' Loss Landscape has been accepted to NeurIPS 2019! </li>
    <li> (06.2019) Received top 5% reviewer award for ICML 2019 :) </li>
    <li> (03.2019) Our <a href="https://arxiv.org/abs/1902.00751">paper</a> on parameter efficient training of BERT was accepted to ICML 2019!</li>
    <li> (01.2019) Papers accepted to ICLR 2019, and AISTATS 2019! Also, our preprint on Neural Architecture Search is <a href="https://arxiv.org/abs/1812.10666">online</a>.  </li> -->
</ul>


<p> Current students </p>
<ul>
    <li> [PhD] Łukasz Maziarka (UJ), co-advised with Jacek Tabor </li>
    <li> [PhD] Mateusz Pyla (UJ & IDEAS), co-advised with Tomasz Trzciński </li>
</ul>

<p> Previous students </p>
<ul>
    <li> [MSc] Piotr Helm (UJ) </li>
    <li> [MSc] Bartosz Krzepkowski (UW) </li>
    <li> [MSc] Przemysław Kaleta (PW) - Speeding-up retrosynthesis, co-advised with Piotr Miłoś </li>
    <li> [MSc] Aleksandra Talar (UJ) - Out-of-distribution generalization in molecule property prediction </li>
    <li> [PhD] Maciej Szymczak (UJ), co-advised with Jacek Tabor </li>
    <li> [MSc] Sławomir Mucha (UJ) - Pretraining in deep learning in cheminformatics </li>
    <li> [MSc] Tobiasz Ciepliński (UJ) - Evaluating generative models in chemistry using docking simulators </li>
    <li> [BSc] Michał Zmysłowski (UW) - Is noisy quadratic model of training of deep neural networks realistic enough? </li>
    <li> [MSc] Olivier Astrand (NYU) - Memorization in deep learning </li>
    <li> [MSc] Tomasz Wesołowski (UJ) - Relevance of enriching word embeddings in modern deep natural language processing </li>
    <li> [MSc] Andrii Krutsylo (UJ) - Physics aware representation for drug discovery </li>
    <li> [BSc] Michał Soboszek (UJ) - Evaluating word embeddings</li>
    <li> [MSc] Jakub Chłędowski (UJ) - Representation learning for textual entailment</li>
    <li> [MSc] Mikołaj Sacha (UJ) - Meta learning and sharpness of the minima </li>
</ul>


<h2 id="publications"> Selected Publications </h2>

<p> For a full list please see my <a href="https://scholar.google.pl/citations?user=wbJxGQ8AAAAJ&hl=pl">Google Scholar profile</a>. </p>


<cells>
    {% for entry in site.data.papers %}
    <div class="cell">
        <div class="cell-image">
            <img src="{{ entry.img }}" alt="Logo image">
        </div>
        <div class="cell-content">
            <h1>{{ entry.title }}</h1>

            <p class="small-margin"> {{ entry.content }} </p>

            <p class="cell-detail">
                {% if entry.details %}
                {{ entry.details }} <br>
                {% endif %}

                {% if entry.github %}
                <a href="{{ entry.github }}" class="flat"><i class="fa fa-github"></i> code </a>
                {% endif %}

                {% if entry.url %}
                <a href="{{ entry.url }}" class="flat"><i class="fa fa-link"></i> url </a>
                {% endif %}

                {% if entry.pdf %}
                <a href="{{ entry.pdf }}" class="flat"> paper </a>
                {% endif %}

                {% if entry.poster %}
                <a href="{{ entry.poster }}" class="flat"> poster </a>
                {% endif %}

                {% if entry.talk %}
                <a href="{{ entry.talk }}" class="flat"> talk </a>
                {% endif %}


                {% if entry.slides %}
                <a href="{{ entry.slides }}" class="flat"> slides </a>
                {% endif %}
            </p>
        </div>
    </div>
    {% endfor %}
</cells>

